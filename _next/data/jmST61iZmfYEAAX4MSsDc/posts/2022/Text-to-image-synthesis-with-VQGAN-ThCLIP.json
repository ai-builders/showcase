{"pageProps":{"postData":{"id":"Text-to-image-synthesis-with-VQGAN-ThCLIP","content":"\n![image](/images/2022/32/01.jpg)\n\n- โมเดลสร้างรูปภาพจากคำอธิบายภาษาไทยเพื่อสร้างภาพประกอบนิยาย เรื่องสั้น หรือบทความต่าง ๆ; เลือกใช้ VQGAN และ CLIP โดย VQGAN จะทำหน้าที่เป็นเสมือนผู้วาดรูปและ CLIP จะทำหน้าที่เป็นคนที่คอยกำกับรูปที่ VQGAN วาดว่าตรงกับข้อความที่เราวาดไปแค่ไหน,\n- CLIP เป็นโมเดลที่เป็นสะพานเชื่อมระหว่างรูปภาพกับข้อความโดยจะทำการ Embed ทั้งสองอย่างนี้ให้อยู่ใน latent space ขนาดเท่ากันจึงสามารถนำทั้ง Text embedding และ Image embedding มาเปรียบเทียบความเหมือนความต่างได้โดยใช้วิธีการทางคณิตศาตร์ต่าง ๆ CLIP จะประกอบไปด้วยโมเดลหลัก 2 ส่วนคือ Text encoder และ Image encoder,\n- VQGAN เป็นโมเดลประเภท Generative โดยจะประกอบไปด้วย Generator และ Discriminator โมเดลสองตัวนี้จะแข่งกันและพัฒนาตัวเองไปพร้อม ๆ กันจนในที่สุด Discriminator ไม่สามารถเอาชนะ Generator ได้ เราจึงจะนำ Generator ไปสร้างภาพต่อ นอกจากนี้ก่อนที่ทั้งสองโมเดลนี้จะแข่งกัน Generator จะได้เรียนรู้โครงสร้างของภาพที่ตัวเองจะสร้างไว้ก่อน เปรียบเสมือนมีสมุดเปล่าที่เอาไว้จดวิธีการสร้างสิ่งต่าง ๆ ลงไป,\n- เนื่องจากการจะสร้างรูปจาก VQGAN นั้นต้องมี Random input (กลุ่มตัวเลขแบบสุ่ม) ส่งเข้าไปให้ Generator จึงจะสร้างเป็นรูปต่าง ๆ ได้และแน่นอนว่าเราไม่สามารถรู้ได้ว่าการสุ่มแบบไหนจะให้รูปที่เราต้องการได้ จึงต้องใช้ CLIP คอยกำกับและค่อย ๆ เปลี่ยน Random input เหล่านี้ให้เป็นรูปที่เราต้องการ,\n- วิธี #1: เริ่มจากการเทรน CLIP ขึ้นมาเองใช้ข้อมูลเริ่มจาก flicker8k (8,000 รูป) ไปจนถึง flicker30k (30,000 รูป) แปลข้อมูล caption จากภาษาอังกฤษไปไทยด้วย PyThaiNLP; ใช้ resnet-50 เป็น image encoder และ WangchanBERTa เป็น text encoder (ThCLIP),\n- วิธีแรกยังไม่ได้ผลเป็นที่น่าพึงพอใจด้วยปัจจัยขนาดชุดข้อมูลและเวลาในการเทรน จึงทดลองวิธี knowledge distillation สำหรับ CLIP แทน,\n- วิธี #2: เทรนใหม่โดยการสอนให้ ThCLIP สร้าง text embeddings ให้คล้ายกับ OpenAI CLIP มากที่สุดตาม mean-squared error loss; ใช้ปริมาณข้อมูลน้อยกว่าและได้ผลดีกว่ามาก,\n- เทรนตามวิธี #2 ด้วยข้อมูล 2 ล้านรูป สุ่มจาก GCC, MSCOCO เหมือนของ SwedishCLIP,\n- จากการ \"ทดลอง ทดลอง ทดลอง\" พบว่า transform ภาพโดยสุ่มเพิ่มระดับ sharpness ให้กับภาพทำให้กราฟ Loss converge เร็วขึ้น, iteration ที่เหมาะที่สุดอยู่ระหว่าง 200–300 รอบ, รวมถึง:,\n- ใช้ negative prompt เพื่อพัฒนาคุณภาพรูปภาพได้ เช่น เมื่อสร้างภาพ \"เครื่องบิน\" ให้ \"ภาพเบลอ\" เป็น negative prompt (ใส่เข้าไปใน loss function) จะทำให้ได้ภาพเครื่องบินที่ชัดขึ้น,\n- ใช้ aesthetic-predictor จาก pretrained model ที่ประเมิน \"ความสวย\" ของรูป และให้ความสวยเป็นส่วนหนึ่งของ loss function; ทำให้รูปที่ออกมาสวยขึ้น,\n- ทดสอบประสิทธิภาพโมเดลด้วยแบบสอบถามจาก 27 คน พบว่า VGQAN-ThCLIP (โมเดลของเรา) ทำได้ดีกว่า VQGAN-CLIP (pretrained จาก OpenAI) เมื่อรูปภาพถูกสร้างจากข้อความสั้นทั้งด้านความสอดคล้องและความหลากหลาย แต่ยิ่งข้อความยาวทำได้ดีกว่าในด้านความสอดคล้องแต่ด้อยกว่าในด้านความหลากหลาย,\n- โมเดลสามารถทำ prompting ได้เหมือน text-to-image ชั้นนำทั่วไป เช่น \"คฤหาสน์\", \"คฤหาสน์ แฟนตาซี\", \"คฤหาสน์ แฮรี่พอตเตอร์\" ฯลฯ\n\n### แรงจูงในในการเข้าร่วมโครงการ (จากใบสมัครเข้าร่วมเมื่อ 10 สัปดาห์ที่แล้ว)\n\n> \"ผู้เขียนมีความสนใจทางด้าน machine learning เป็นอย่างมากและยังมีความมุ่งมั่นที่จะทำให้สำเร็จ ปัจจุบัน machine learning & AI มีบทบาทกับเราแทบทุกด้าน แต่การจะทำให้ AI ใช้งานได้ในแต่ละด้านนั้นแตกต่างกันอย่างสิ้นเชิง ผู้เขียนจึงอยากเก็บเกี่ยวประสบการณ์ตั้งแต่การสร้างไปจนถึงการนำไปใช้ของ Machine learning & AI ในแต่ละด้านเพื่อขยายขอบเขตไอเดียของตัวเองและเมื่อไอเดียที่เข้าท่ามาถึง ผู้เขียนก็สามารถลงมือทำได้เลย โดยรู้ว่าด้านที่จะทำนั้นต้องใช้ model ประมาณนี้ รู้แหล่งชุดข้อมูลและรู้คีย์เวิร์ดที่จะศึกษาเพิ่มเติม นอกจากนั้นการเข้าโครงการนี้จะทำให้เราได้เจอกับเมนเทอร์และเพื่อนร่วมโครงการที่มีความชอบ เหมือนๆ กัน เมื่อเค้ามีความถนัดในด้าน machine learing ที่เราจะทำเราก็สามารถขอคำแนะนำจากเค้าได้และในทางกลับกันเราก็สามารถให้คำแนะนำในด้านที่เราถนัดแก่เพื่อนร่วมโครงการได้เช่นกัน เป้าหมายของผู้เขียนคือการสร้างนวัตกรรมหรือสิ่งประดิษฐ์ที่ทุกคนสามารถใช้ได้อย่างแพร่หลายและเกิดประโยชน์จากการใช้งานนั้น ซึ่งปัจจุบันผู้เขียนอยากศึกษาเพิ่มเติมทางด้าน Machine learning & AI อยากลองศึกษาศาสตร์แขนงใหม่ๆของ machine learing อาทิเข่น NLP, time series, GAN และอยากศึกษาวิธีการนำไปปรับใช้กับปัญหาในชีวิตจริง  อยากทำการสร้างรูปสังเคราะห์จากข้อความภาษาไทย (text to image genarator) เพื่อจำลองการออกแบบผลิตภัณฑ์เบื้องต้นจากไอเดียแล้วนำไปประเมิณว่าสามารถนำไปต่อยอดได้มากเพียงใดหรือเพื่อค้นพบไอเดียใหม่จากรูปที่สังเคราะห์ขึ้นมา เช่น อยากออกแบบดีไซน์เก้าอี้จึงพิมพ์ไปว่าเก้าอี้ทรงทุเรียน ตัวโมเดลอาจจะให้ผลลัพธ์เป็นเก้าอี้ที่มีหนามทุเรียนเป็นขาหรือเก้าอี้ที่ใช้หนามมนเป็นที่นั่ง และอื่นๆที่ผู้เขียนคาดไม่ถึง ชุดข้อมูลรูป ใช้จาก COCO โดยส่วนมากจะเป็นรูปสิ่งของต่างๆ, ชุดข้อมูลคำ ใช้คำที่ผ่านการ embedded มาแล้วโดยใช้ thai2vec ด้วย PyThaiNLP, นอกจากนี้ถ้าเปลี่ยนชุดข้อมูลใหม่โดยใช้รูปหน้าคนแล้วเทรนให้โมเดลสร้างหน้าคนจากคำบอกเล่าของพยานก็จะสามารถสร้างรูปคนร้ายเพื่อใช้ในการตามหาตัวหรือทำรูปติดประกาศจับ การทำแบบนี้จะสามารถลดระยะเวลาการสเก็ตช์ภาพขั้นต้นและสามารถโฟกัสกับการปรับแต่งขั้นสุดท้ายให้ภาพตรงกับคนร้ายที่พยานเห็นมากที่สุด ก็เป็นอีกโครงงานหนึ่งที่น่าทำเช่นกัน\"","date":"12-7-22","title":"Text-to-image synthesis with VQGAN-ThCLIP","builder":"ภูริช ศิริทิพย์ (มาร์ค)","builder_info":"","thumbnail":"/images/2022/32/01.jpg","links":{"github":"https://colab.research.google.com/github/vikimark/VQGAN-ThCLIP/blob/master/Streamlit_VQGANxThaiCLIP.ipynb","facebook":"https://facebook.com/aibuildersx/posts/413224897512622","blog":"https://medium.com/@phuritsiritip/%E0%B9%82%E0%B8%84%E0%B8%A3%E0%B8%87%E0%B8%81%E0%B8%B2%E0%B8%A3-ai-builders-%E0%B8%81%E0%B8%B1%E0%B8%9A-ai-%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87%E0%B8%A0%E0%B8%B2%E0%B8%9E%E0%B8%88%E0%B8%B2%E0%B8%81%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%84%E0%B8%A7%E0%B8%B2%E0%B8%A1%E0%B8%AA%E0%B8%A3%E0%B9%89%E0%B8%B2%E0%B8%87%E0%B9%82%E0%B8%94%E0%B8%A2%E0%B9%80%E0%B8%94%E0%B9%87%E0%B8%81%E0%B8%A1%E0%B8%B1%E0%B8%98%E0%B8%A2%E0%B8%A1%E0%B8%9B%E0%B8%A5%E0%B8%B2%E0%B8%A2-%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B9%80%E0%B8%81%E0%B8%B7%E0%B8%AD%E0%B8%9A%E0%B8%88%E0%B8%B0%E0%B8%82%E0%B8%B6%E0%B9%89%E0%B8%99%E0%B8%9B%E0%B8%B5-1-ed5878c7a72c"}}},"__N_SSG":true}